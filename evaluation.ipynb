{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "jjXd3YoUUcy2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Binary classification"
      ],
      "metadata": {
        "id": "izSd66VUU2p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ev_df = pd.read_csv('deepseek_evaluation.csv')"
      ],
      "metadata": {
        "id": "YIMj3L-9b5bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_word_label(file, label, not_label, deepseek_column):\n",
        "  if 'csv' in file:\n",
        "    df = pd.read_csv(file)\n",
        "  elif 'xlsx' in file:\n",
        "    df = pd.read_excel(file)\n",
        "\n",
        "\n",
        "  df[deepseek_column] = df[deepseek_column].astype(str)\n",
        "  df['deepseek_final'] = 'None'\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    if not_label in row[deepseek_column]:\n",
        "      df.loc[index, 'deepseek_final'] = not_label\n",
        "    elif label in row[deepseek_column]:\n",
        "      df.loc[index, 'deepseek_final'] = label\n",
        "\n",
        "  df_filtered = df[df.deepseek_final != 'None']\n",
        "\n",
        "  gold = df_filtered[\"annotation\"].tolist()\n",
        "  system = df_filtered[\"deepseek_final\"].tolist()\n",
        "\n",
        "  f1_macro = f1_score(gold, system, average='macro')\n",
        "  f1_micro = f1_score(gold, system, average='micro')\n",
        "  accuracy = accuracy_score(gold, system)\n",
        "\n",
        "  print(f'Accuracy: {accuracy}')\n",
        "  print(f'F1-macro: {f1_macro}')\n",
        "  print(f'F1-micro: {f1_micro}')\n",
        "  print(f'None: {len(df) - len(df_filtered)}')\n",
        "\n",
        "  filename = file.split('.')[0]\n",
        "  try:\n",
        "    df.to_excel(f'{filename}_final.xlsx', index=False)\n",
        "  except:\n",
        "    df.to_csv(f'{filename}_final.csv', index=False)\n",
        "\n",
        "  ev_df = pd.read_csv('deepseek_evaluation.csv')\n",
        "  task = file.replace('.csv', '').replace('.xlsx', '')\n",
        "  if task not in ev_df['task'].values:\n",
        "    ev_df = pd.concat([\n",
        "        ev_df,\n",
        "        pd.DataFrame([{'task': task, 'accuracy': 0, 'f1_macro': 0}])\n",
        "    ], ignore_index=True)\n",
        "\n",
        "  ev_df.loc[ev_df['task'] == task, ['accuracy', 'f1_macro']] = [\n",
        "    round(accuracy * 100, 2),\n",
        "    round(f1_macro * 100, 2)\n",
        "  ]\n",
        "  ev_df.to_csv('deepseek_evaluation.csv', index=False)"
      ],
      "metadata": {
        "id": "Kt6UkwEwU1lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Aggression"
      ],
      "metadata": {
        "id": "N2HmwbCNVco0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_word_label('aggression.xlsx', 'aggressive', 'non-aggressive', 'deepseek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E5NfQIYVfZJ",
        "outputId": "9b4e91f0-5098-4bc3-8da7-0536919be5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5757271815446339\n",
            "F1-macro: 0.5290095185126864\n",
            "F1-micro: 0.5757271815446339\n",
            "None: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. AggressionPer"
      ],
      "metadata": {
        "id": "lbExyME1DDjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_word_label('aggressionper.xlsx', 'aggressive', 'non-aggressive', 'deepseek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvcEZ9S4DHvi",
        "outputId": "ac31b32f-6aa3-4665-b271-505aee63a15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6863727454909819\n",
            "F1-macro: 0.6460090544912657\n",
            "F1-micro: 0.6863727454909819\n",
            "None: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.Sarcasm"
      ],
      "metadata": {
        "id": "sfp7ZLUID7dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_word_label('sarcasm.xlsx', 'funny', 'not funny', 'deepseek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZszTbtAD8AW",
        "outputId": "a993bbf0-be45-476b-b315-9725a6248226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4894894894894895\n",
            "F1-macro: 0.48948488565038595\n",
            "F1-micro: 0.4894894894894895\n",
            "None: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.ColBERT"
      ],
      "metadata": {
        "id": "j9RPzJEiEdhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_word_label('colbert.xlsx', 'funny', 'not funny', 'deepseek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta89sQziEfr9",
        "outputId": "e233bc39-c195-449b-9265-f26600a62a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.845\n",
            "F1-macro: 0.8446854881134297\n",
            "F1-micro: 0.845\n",
            "None: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.Spam"
      ],
      "metadata": {
        "id": "R2nWOZ9dFg3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_word_label('spam.xlsx', 'spam', 'not spam', 'deepseek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CScJVXPwFijg",
        "outputId": "1304b97f-333d-423c-ded1-ad98815638cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7336322869955157\n",
            "F1-macro: 0.6566352944348073\n",
            "F1-micro: 0.7336322869955157\n",
            "None: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.TextEntail"
      ],
      "metadata": {
        "id": "WsQxx7L6GUGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('textentail2.xlsx')"
      ],
      "metadata": {
        "id": "B_2kbLTaGXIC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['split'] == 'dev']"
      ],
      "metadata": {
        "id": "AiCR_AHZLPdM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['deepseek_final'] = \"None\"\n",
        "for index, row in df.iterrows():\n",
        "    if 'not_entailment' in row['deepseek']:\n",
        "      df.loc[index, 'deepseek_final'] = 'not_entailment'\n",
        "    elif 'entailment' in row['deepseek'] or 'entailemtent' in row['deepseek']:\n",
        "      df.loc[index, 'deepseek_final'] = 'entailment'"
      ],
      "metadata": {
        "id": "xQfBD-vVLc5S"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_MAP = {\n",
        "    \"entailment\": 1,\n",
        "    \"not_entailment\": 0,\n",
        "}"
      ],
      "metadata": {
        "id": "TeQkcASkNVSc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_labels = [LABEL_MAP[val.lower()] for val in df.label.values]\n",
        "deepseek_preds = [LABEL_MAP[val.lower()] for val in df.deepseek_final.values]"
      ],
      "metadata": {
        "id": "6oGC6WcDLkQu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_macro = f1_score(gold_labels, deepseek_preds, average='macro')\n",
        "f1_micro = f1_score(gold_labels, deepseek_preds, average='micro')\n",
        "accuracy = accuracy_score(gold_labels, deepseek_preds)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1-macro: {f1_macro}')\n",
        "print(f'F1-micro: {f1_micro}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-a_fW_fLr27",
        "outputId": "448f6378-2a4d-4637-a58e-ef51266cd5dd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.851985559566787\n",
            "F1-macro: 0.8470004984574762\n",
            "F1-micro: 0.851985559566787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev_df = pd.read_csv('deepseek_evaluation.csv')\n",
        "\n",
        "ev_df.loc[ev_df['task'] == 'textentail', ['accuracy', 'f1_macro']] = [\n",
        "  round(accuracy * 100, 2),\n",
        "  round(f1_macro * 100, 2)\n",
        "]\n",
        "ev_df.to_csv('deepseek_evaluation.csv', index=False)"
      ],
      "metadata": {
        "id": "4Cjs4yNgL5K-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextEntail GPT4\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZZr1NZaNZGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_labels = [LABEL_MAP[val.lower()] for val in df.label.values]\n",
        "gpt4_preds = [LABEL_MAP[val.lower()] for val in df.gpt4.values]"
      ],
      "metadata": {
        "id": "s6HyCyixNTGX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_macro = f1_score(gold_labels, gpt4_preds, average='macro')\n",
        "f1_micro = f1_score(gold_labels, gpt4_preds, average='micro')\n",
        "accuracy = accuracy_score(gold_labels, gpt4_preds)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1-macro: {f1_macro}')\n",
        "print(f'F1-micro: {f1_micro}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jl3GAYoNwMm",
        "outputId": "a93baa59-e093-422c-b702-e0b17a7c3403"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9061371841155235\n",
            "F1-macro: 0.9049466793369232\n",
            "F1-micro: 0.9061371841155235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev_df = pd.read_csv('deepseek_evaluation.csv')\n",
        "\n",
        "ev_df.loc[ev_df['task'] == 'textentail', ['accuracy_gpt4', 'f1_macro_gpt4']] = [\n",
        "  round(accuracy * 100, 2),\n",
        "  round(f1_macro * 100, 2)\n",
        "]\n",
        "ev_df.to_csv('deepseek_evaluation.csv', index=False)"
      ],
      "metadata": {
        "id": "pEQsUkB9ODrv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.Cola"
      ],
      "metadata": {
        "id": "a1ibmUfAO30f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cola = pd.read_excel('cola.xlsx')"
      ],
      "metadata": {
        "id": "TQG-JxxFQBFc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cola_filtered = cola[cola.deepseek.isin([0, 1])]"
      ],
      "metadata": {
        "id": "gbNOqU-aQPxL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_labels = cola_filtered['annotation'].tolist()\n",
        "deepseek_preds = cola_filtered['deepseek'].tolist()"
      ],
      "metadata": {
        "id": "VtbUIpBUQSTa"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_macro = f1_score(gold_labels, deepseek_preds, average='macro')\n",
        "f1_micro = f1_score(gold_labels, deepseek_preds, average='micro')\n",
        "accuracy = accuracy_score(gold_labels, deepseek_preds)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1-macro: {f1_macro}')\n",
        "print(f'F1-micro: {f1_micro}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPUc_h8tQYRP",
        "outputId": "2797e618-f3de-4709-da90-1e341a783759"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8559077809798271\n",
            "F1-macro: 0.8292994883903975\n",
            "F1-micro: 0.8559077809798271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev_df.loc[ev_df['task'] == 'cola', ['accuracy', 'f1_macro']] = [\n",
        "  round(accuracy * 100, 2),\n",
        "  round(f1_macro * 100, 2)\n",
        "]\n",
        "ev_df.to_csv('deepseek_evaluation.csv', index=False)"
      ],
      "metadata": {
        "id": "dlQzEM_WQq4q"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.WSD"
      ],
      "metadata": {
        "id": "Utz1W0peRSA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('wsd.xlsx')"
      ],
      "metadata": {
        "id": "ueO0wdszRUTn"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['deepseek'] = df['deepseek'].astype(str)\n",
        "df['deepseek_final'] = 'None'\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['annotation'] in row['deepseek']:\n",
        "        df.loc[index, 'deepseek_final'] = row['annotation']\n",
        "    elif row['deepseek'].strip() != '':\n",
        "        df.loc[index, 'deepseek_final'] = 'wrong'\n",
        "\n",
        "gold_labels = df['annotation'].tolist()\n",
        "deepseek_preds = df['deepseek_final'].tolist()\n",
        "\n",
        "f1_macro = f1_score(gold_labels, deepseek_preds, average='macro')\n",
        "f1_micro = f1_score(gold_labels, deepseek_preds, average='micro')\n",
        "accuracy = accuracy_score(gold_labels, deepseek_preds)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1-macro: {f1_macro}')\n",
        "print(f'F1-micro: {f1_micro}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rh9u4myR2EV",
        "outputId": "404b6d0f-7883-4290-818c-04b5c0351850"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7836757203915621\n",
            "F1-macro: 0.772345014123017\n",
            "F1-micro: 0.7836757203915621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev_df.loc[ev_df['task'] == 'wsd', ['accuracy', 'f1_macro']] = [\n",
        "  round(accuracy * 100, 2),\n",
        "  round(f1_macro * 100, 2)\n",
        "]\n",
        "ev_df.to_csv('deepseek_evaluation.csv', index=False)"
      ],
      "metadata": {
        "id": "FtthdkEiSaWR"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9.ClarinEmo"
      ],
      "metadata": {
        "id": "AEVj-0fgSpyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ast\n",
        "\n",
        "from torchmetrics import Accuracy, F1Score, Precision, Recall\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "w5D0x-qBSsK4"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels = ['radość', 'zaufanie', 'przeczuwanie', 'zdziwienie', 'strach', 'smutek', 'wstręt', 'gniew', 'pozytywny', 'negatywny', 'neutralny']\n",
        "\n",
        "num_labels = len(emotion_labels)"
      ],
      "metadata": {
        "id": "0wmo7Hn8Svtt"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macro_prec = Precision(num_labels=num_labels, average=\"macro\", task='multilabel')\n",
        "macro_recall = Recall(num_labels=num_labels, average=\"macro\", task='multilabel')\n",
        "micro_prec = Precision(num_labels=num_labels, average=\"micro\", task='multilabel')\n",
        "micro_recall = Recall(num_labels=num_labels, average=\"micro\", task='multilabel')\n",
        "macro_f1 = F1Score(num_labels=num_labels, average='macro', task='multilabel')\n",
        "weighted_f1 = F1Score(num_labels=num_labels, average='weighted', task='multilabel')\n",
        "micro_f1 = F1Score(num_labels=num_labels, average='micro', task='multilabel')\n",
        "f1_scores = F1Score(num_labels=num_labels, average='none', task='multilabel')\n",
        "accuracy = Accuracy(num_labels=num_labels, task='multilabel')"
      ],
      "metadata": {
        "id": "tvt0L6IiThoi"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('clarinemo.xlsx')"
      ],
      "metadata": {
        "id": "ADx8KuczTlOu"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_json_cell(cell):\n",
        "    if isinstance(cell, str):\n",
        "        cleaned = re.sub(r'^```json\\s*|\\s*```$', '', cell.strip())\n",
        "        try:\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке строки: {cell}\\n{e}\")\n",
        "            return None\n",
        "    return cell\n",
        "\n",
        "\n",
        "df['deepseek'] = df['deepseek'].apply(clean_json_cell)"
      ],
      "metadata": {
        "id": "IooefoHqTtCv"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_output(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    elif isinstance(x, dict):\n",
        "        annotations = set()\n",
        "        for element in x.values():\n",
        "            [annotations.add(e) for e in element]\n",
        "        return list(annotations)\n",
        "    assert(False)"
      ],
      "metadata": {
        "id": "5A8qsQBuTxga"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "indexes = []\n",
        "for index, row in df.iterrows():\n",
        "    annotations = ast.literal_eval(row.annotation)\n",
        "    y_true_example = [[x in anno for x in emotion_labels] for anno in annotations]\n",
        "    chat_gpt =  ast.literal_eval(row.deepseek)\n",
        "    sentences = chat_gpt.keys()\n",
        "    sentences = sorted(sentences)\n",
        "    y_pred_example = [handle_output(chat_gpt[key]) for key in sentences]\n",
        "    y_pred_example = [[x in anno for x in emotion_labels] for anno in y_pred_example]\n",
        "    if len(y_true_example) != len(y_pred_example):\n",
        "        continue\n",
        "    [y_true.append(example) for example in y_true_example]\n",
        "    [y_pred.append(example) for example in y_pred_example]"
      ],
      "metadata": {
        "id": "563RHq-WT0sz"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = torch.Tensor(y_true)\n",
        "y_pred = torch.Tensor(y_pred)"
      ],
      "metadata": {
        "id": "OJSzXMrtT302"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_names = [k.capitalize() + ' f1:' for k in emotion_labels]\n",
        "emotion_f1 = dict(zip(f1_names, f1_scores(y_pred, y_true)))"
      ],
      "metadata": {
        "id": "QdZ4L8TKT6Z7"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = float(accuracy(y_pred, y_true) * 100)\n",
        "f1_macro_score = float(macro_f1(y_pred, y_true) * 100)\n",
        "f1_micro_score = float(micro_f1(y_pred, y_true) * 100)\n",
        "print(f'Accuracy: {acc}')\n",
        "print(f'F1-macro: {f1_macro_score}')\n",
        "print(f'F1-micro: {f1_micro_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59oxpV9JT-H9",
        "outputId": "56f043b2-7f05-4f59-f5bc-1e2ffb51991f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.26692962646484\n",
            "F1-macro: 53.890533447265625\n",
            "F1-micro: 63.449363708496094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev_df.loc[ev_df['task'] == 'clarinemo', ['accuracy', 'f1_macro']] = [\n",
        "  round(acc, 2),\n",
        "  round(f1_macro_score, 2)\n",
        "]\n",
        "ev_df.to_csv('deepseek_evaluation.csv', index=False)"
      ],
      "metadata": {
        "id": "hWzn-2K8Vdy7"
      },
      "execution_count": 111,
      "outputs": []
    }
  ]
}